{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f348f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, tiktoken, numpy as np, os, datetime, uuid\n",
    "from typing import Literal, Optional, Tuple\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader, YoutubeLoader\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "learn_api_key = os.environ['LANGSMITH_API_KEY']\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "anthropic_api_key = os.environ['ANTHROPIC_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018c051",
   "metadata": {},
   "source": [
    "### Multi-representation Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e40e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef48790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"doc\" : lambda x:x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=groq_api_key, temperature=0, max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7bd850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15232\\4133976571.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(collection_name=\"summaries\", embedding_function=HuggingFaceEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=HuggingFaceEmbeddings())\n",
    "# sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "docstore = {}  # Use a dict for docstore\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key:doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2d89381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '9c48de2f-dd97-4670-9607-c3d49fd318d3'}, page_content=\"The document discusses the concept of LLM (Large Language Model) powered autonomous agents, which are systems that use LLMs as their core controller. The author, Lilian Weng, explores the key components of such agents, including planning, memory, and tool use.\\n\\n**Planning**: The author discusses various techniques for planning in LLM-powered agents, including task decomposition, self-reflection, and chain of thought (CoT) prompting. CoT involves breaking down complex tasks into smaller, manageable steps, while self-reflection allows the agent to learn from its mistakes and refine its plans.\\n\\n**Memory**: The author explains that memory can be categorized into short-term and long-term memory. Short-term memory refers to the limited context window of the LLM, while long-term memory involves storing information in an external vector store that can be retrieved using maximum inner product search (MIPS) algorithms.\\n\\n**Tool Use**: The author discusses the importance of tool use in LLM-powered agents, which involves using external APIs and tools to extend the agent's capabilities. The author mentions several examples of tool-augmented LLMs, including MRKL, TALM, and Toolformer.\\n\\n**Case Studies**: The author presents several case studies of LLM-powered agents, including:\\n\\n1. **Scientific Discovery Agent**: An agent that uses LLMs to plan and execute scientific experiments.\\n2. **Generative Agents Simulation**: A simulation of virtual characters controlled by LLM-powered agents that interact with each other and their environment.\\n3. **AutoGPT**: A proof-of-concept demo of an LLM-powered agent that can perform tasks autonomously.\\n4. **GPT-Engineer**: A project that uses LLMs to generate code for a given task.\\n\\n**Challenges**: The author highlights several challenges in building LLM-powered autonomous agents, including:\\n\\n1. **Finite context length**: The limited context window of LLMs restricts the amount of information that can be processed.\\n2. **Long-term planning and task decomposition**: Planning over a lengthy history and effectively exploring the solution space remain challenging.\\n3. **Reliability of natural language interface**: The reliability of model outputs is questionable, and LLMs may make formatting errors or exhibit rebellious behavior.\\n\\nOverall, the document provides a comprehensive overview of the current state of LLM-powered autonomous agents, highlighting both the potential benefits and challenges of this technology.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "\n",
    "sub_docs = vectorstore.similarity_search(query, k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c365354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15232\\17140411.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query, n_results=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three:\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query, n_results=1)\n",
    "retrieved_docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27bc84",
   "metadata": {},
   "source": [
    "### RAPTOR (Hierarchical Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8f9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL with PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader = RecursiveUrlLoader(url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL with Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# Doc texts\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37cbe101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2e84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 3555\n"
     ]
    }
   ],
   "source": [
    "# Doc texts concat\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join([doc.page_content for doc in d_reversed])\n",
    "print(f\"Num tokens in all context: {num_tokens_from_string(concatenated_content, 'cl100k_base')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8afff4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Doc texts split\n",
    "chunk_size_tok = 2000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size_tok, chunk_overlap=0)\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5766b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "embd = HuggingFaceEmbeddings()\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=groq_api_key, temperature=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7815db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np, pandas as pd, umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224\n",
    "\n",
    "def global_cluster_embeddings(embeddings: np.ndarray, dim: int, n_neighbors: Optional[int] = None, metric: str = \"cosine\",)-> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "    \n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neioghbors: Optional; the number of neighbors to consider for each point.\n",
    "                    If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(n_neighbors=n_neighbors, n_components=dim, metric=metric).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "705c265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_cluster_embeddings(embeddings: np.ndarray, dim: int, n_neighbors: int = 10, metric: str = \"cosine\",)-> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neioghbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(n_neighbors=n_neighbors, n_components=dim, metric=metric).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0a005c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_clusters(embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED) -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture\n",
    "    \n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum nbumber of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - Am integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf04d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353ce6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(embeddings: np.ndarray, dim: int, threshold: float,) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering \n",
    "    a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "    \n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(reduced_embeddings_global, threshold)\n",
    "\n",
    "    all_local_clusters = [np.ndarray([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[np.array([i in gc for gc in global_clusters])]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(global_cluster_embeddings_, dim)\n",
    "            local_clusters, n_local_clusters = GMM_cluster(reduced_embeddings_local, threshold)\n",
    "        \n",
    "        # Assign local cluster IDs, adjusting for total cluster already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[np.array([j in lc for lc in local_clusters])]\n",
    "            indices = np.where((embeddings == local_cluster_embeddings_[:, None]).all(-1))[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(all_local_clusters[idx], j + total_clusters)\n",
    "        total_clusters += n_global_clusters\n",
    "    return all_local_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00303396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "    \n",
    "    This function assumes the existence of an 'embd' object with a method 'embed_documents'\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f63b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "    \n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence of a previously\n",
    "    defined 'perform_clustering' function that performs clustering on the embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "    \n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(text_embeddings_np, 10, 0.1)  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)\n",
    "    df[\"cluster\"] = cluster_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b36de3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba0e5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize_texts(texts: List[str], level: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes \n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame ('df_clusters') includes the original texts, their embeddings, and cluster assignment.\n",
    "      2. The second DataFrame ('df_summary') contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append({\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster})\n",
    "    \n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Language doc.\n",
    "    \n",
    "    LangChain Expression Language provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Format text within eaech cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame({\"summaries\":summaries, \"level\": [level]* len(summaries), \"cluster\": list(all_clusters),})\n",
    "    return df_clusters, df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6df197eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_embed_cluster_summarize(texts: List[str], level: int = 1, n_levels: int = 3) -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until the number of unique clusters becomes 1,\n",
    "    storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion levels and values are tuples containing\n",
    "      the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input text for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(new_texts, level + 1, n_levels)\n",
    "\n",
    "        # Merge the result from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1380bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b64c2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extent all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Now, use all_texts to build the vectorstore with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=embd)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8de80ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To define a RAG (Retrieval Augmented Generation) chain, you can use the LangChain Expression Language (LCEL) to compose existing Runnables together. A specific code example is not provided in the given context, but you can use the `RunnableSequence` and `RunnableParallel` composition primitives to build a RAG chain. For example, you can use `RunnableSequence` to chain multiple runnables sequentially, with the output of one runnable serving as the input to the next.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Propmt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = ({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser())\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How to define a RAG chain? Give me a specific code example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6733e55",
   "metadata": {},
   "source": [
    "### ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd44c799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15232\\276443619.py:1: UserWarning: \n",
      "********************************************************************************\n",
      "RAGatouille WARNING: Future Release Notice\n",
      "--------------------------------------------\n",
      "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
      "instead of the current Stanford ColBERT backend.\n",
      "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
      "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
      "********************************************************************************\n",
      "  from ragatouille import RAGPretrainedModel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bce2cc8cdd4db88abc1fc44bc3dfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "artifact.metadata: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--colbert-ir--colbertv2.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e25c272e6354207a6ee2876ba611e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039eb5428a454f69b9c69f901bc4a9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311217bca32e4023b1358e60648841a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcbb4e4bd88432c90a130b681e82c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8207ff6928740349c0afcbe383302ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5438a6846a27440ab1e4c845238f9dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 17, 00:23:13] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 00:23:13.998000 15232 Lib\\site-packages\\torch\\utils\\cpp_extension.py:466] Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['where', 'cl']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPretrainedModel\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m RAG = \u001b[43mRAGPretrainedModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolbert-ir/colbertv2.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\ragatouille\\RAGPretrainedModel.py:71\u001b[39m, in \u001b[36mRAGPretrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, n_gpu, verbose, index_root)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load a ColBERT model from a pre-trained checkpoint.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[33;03m    cls (RAGPretrainedModel): The current instance of RAGPretrainedModel, with the model initialised.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m instance = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m instance.model = \u001b[43mColBERT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\ragatouille\\models\\colbert.py:84\u001b[39m, in \u001b[36mColBERT.__init__\u001b[39m\u001b[34m(self, pretrained_model_name_or_path, n_gpu, index_name, verbose, load_from_index, training_mode, index_root, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.root = \u001b[38;5;28mself\u001b[39m.index_root\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training_mode:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28mself\u001b[39m.inference_ckpt = \u001b[43mCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolbert_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_max_tokens = (\n\u001b[32m     88\u001b[39m         \u001b[38;5;28mself\u001b[39m.inference_ckpt.bert.config.max_position_embeddings\n\u001b[32m     89\u001b[39m     ) - \u001b[32m4\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mself\u001b[39m.run_context = Run().context(\u001b[38;5;28mself\u001b[39m.run_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\colbert\\modeling\\checkpoint.py:75\u001b[39m, in \u001b[36mCheckpoint.__init__\u001b[39m\u001b[34m(self, name, colbert_config, verbose)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, colbert_config=\u001b[38;5;28;01mNone\u001b[39;00m, verbose: \u001b[38;5;28mint\u001b[39m = \u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolbert_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.verbose = verbose\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\colbert\\modeling\\colbert.py:24\u001b[39m, in \u001b[36mColBERT.__init__\u001b[39m\u001b[34m(self, name, colbert_config)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(name, colbert_config)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.use_gpu = colbert_config.total_visible_gpus > \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mColBERT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtry_load_torch_extensions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.colbert_config.mask_punctuation:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mself\u001b[39m.skiplist = {w: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     28\u001b[39m                      \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m string.punctuation\n\u001b[32m     29\u001b[39m                      \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m [symbol, \u001b[38;5;28mself\u001b[39m.raw_tokenizer.encode(symbol, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\colbert\\modeling\\colbert.py:39\u001b[39m, in \u001b[36mColBERT.try_load_torch_extensions\u001b[39m\u001b[34m(cls, use_gpu)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     38\u001b[39m print_message(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m segmented_maxsim_cpp = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msegmented_maxsim_cpp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpathlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[34;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msegmented_maxsim.cpp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-O3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCOLBERT_LOAD_TORCH_EXTENSION_VERBOSE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFalse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mcls\u001b[39m.segmented_maxsim = segmented_maxsim_cpp.segmented_maxsim_cpp\n\u001b[32m     51\u001b[39m \u001b[38;5;28mcls\u001b[39m.loaded_extensions = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1681\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_python_module, is_standalone, keep_intermediates)\u001b[39m\n\u001b[32m   1572\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(name,\n\u001b[32m   1573\u001b[39m          sources: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m   1574\u001b[39m          extra_cflags=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1584\u001b[39m          is_standalone=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1585\u001b[39m          keep_intermediates=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1586\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1587\u001b[39m \u001b[33;03m    Load a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[32m   1588\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1679\u001b[39m \u001b[33;03m        ...     verbose=True)\u001b[39;00m\n\u001b[32m   1680\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1681\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msources\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_build_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2138\u001b[39m, in \u001b[36m_jit_compile\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_python_module, is_standalone, keep_intermediates)\u001b[39m\n\u001b[32m   2134\u001b[39m                 hipified_sources.add(hipify_result[s_abs].hipified_path \u001b[38;5;28;01mif\u001b[39;00m s_abs \u001b[38;5;129;01min\u001b[39;00m hipify_result \u001b[38;5;28;01melse\u001b[39;00m s_abs)\n\u001b[32m   2136\u001b[39m             sources = \u001b[38;5;28mlist\u001b[39m(hipified_sources)\n\u001b[32m-> \u001b[39m\u001b[32m2138\u001b[39m         \u001b[43m_write_ninja_file_and_build_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m            \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2144\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2148\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2149\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2150\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2151\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m verbose:\n\u001b[32m   2152\u001b[39m     logger.debug(\u001b[33m'\u001b[39m\u001b[33mNo modifications detected for re-loaded extension module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, skipping build step...\u001b[39m\u001b[33m'\u001b[39m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2275\u001b[39m, in \u001b[36m_write_ninja_file_and_build_library\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_standalone)\u001b[39m\n\u001b[32m   2271\u001b[39m     os.makedirs(build_directory, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2273\u001b[39m \u001b[38;5;66;03m# NOTE: Emitting a new ninja build file does not cause re-compilation if\u001b[39;00m\n\u001b[32m   2274\u001b[39m \u001b[38;5;66;03m# the sources did not change, so it's ok to re-emit (and it's fast).\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2275\u001b[39m \u001b[43m_write_ninja_file_to_build_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2278\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   2289\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mBuilding extension module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2762\u001b[39m, in \u001b[36m_write_ninja_file_to_build_library\u001b[39m\u001b[34m(path, name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, with_cuda, with_sycl, is_standalone)\u001b[39m\n\u001b[32m   2759\u001b[39m ext = EXEC_EXT \u001b[38;5;28;01mif\u001b[39;00m is_standalone \u001b[38;5;28;01melse\u001b[39;00m LIB_EXT\n\u001b[32m   2760\u001b[39m library_target = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m \u001b[43m_write_ninja_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpost_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcuda_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_dlink_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2769\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43msycl_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2770\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2771\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_dlink_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43msycl_dlink_post_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2772\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Projects\\LangChain Tutorials\\learn\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2946\u001b[39m, in \u001b[36m_write_ninja_file\u001b[39m\u001b[34m(path, cflags, post_cflags, cuda_cflags, cuda_post_cflags, cuda_dlink_post_cflags, sycl_cflags, sycl_post_cflags, sycl_dlink_post_cflags, sources, objects, ldflags, library_target, with_cuda, with_sycl)\u001b[39m\n\u001b[32m   2944\u001b[39m link_rule = [\u001b[33m'\u001b[39m\u001b[33mrule link\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IS_WINDOWS:\n\u001b[32m-> \u001b[39m\u001b[32m2946\u001b[39m     cl_paths = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2947\u001b[39m \u001b[43m                                        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.decode(*SUBPROCESS_DECODE_ARGS).split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m   2948\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cl_paths) >= \u001b[32m1\u001b[39m:\n\u001b[32m   2949\u001b[39m         cl_path = os.path.dirname(cl_paths[\u001b[32m0\u001b[39m]).replace(\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m$:\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:466\u001b[39m, in \u001b[36mcheck_output\u001b[39m\u001b[34m(timeout, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m         empty = \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    464\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m] = empty\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m           \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.stdout\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['where', 'cl']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full context of a Wikipedia page.\n",
    "    \n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en/wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad21c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG.index(collection=[full_document], index_name=\"Miyazaki-123\",max_document_length=180, split_documents=True,)\n",
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
