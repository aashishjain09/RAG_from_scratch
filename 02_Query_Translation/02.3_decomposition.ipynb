{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a7840e",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94e2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, tiktoken, numpy as np, os\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.load import dumps, loads\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "learn_api_key = os.environ['LANGCHAIN_API_KEY']\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "groq_api_key = os.environ['GROQ_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdea7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INDEXING ###\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Make Splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(\n",
    "    splits,\n",
    "    HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c258447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768bc909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. “LLM‑powered autonomous agent architecture components”  ',\n",
       " '2. “Key modules in a large‑language‑model autonomous agent system”  ',\n",
       " '3. “Design elements of an autonomous agent powered by a large language model”']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatGroq(model='openai/gpt-oss-20b', api_key=groq_api_key, temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = (prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split('\\n')))\n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a02099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4037d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model='openai/gpt-oss-20b', api_key=groq_api_key, temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0a26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q, \"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a330ab14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Design Elements of an Autonomous Agent Powered by a Large Language Model\\n\\nBelow is a concise, modular blueprint that captures the essential building blocks of a robust LLM‑driven autonomous agent.  \\nThe table and accompanying notes synthesize the key modules from the background Q&A, Lilian\\u202fWeng’s 2023 article, and the broader literature on LLM agents.\\n\\n| **Element** | **Primary Responsibility** | **Typical Implementation** | **Key Design Considerations** |\\n|------------|---------------------------|---------------------------|------------------------------|\\n| **LLM Controller (Brain)** | Generates decisions, plans, and responses. | GPT‑4, Claude‑3, Llama‑2, etc. | Prompt‑engineering, system messages, and token‑budget management. |\\n| **Planning / Goal‑Decomposition** | Breaks high‑level goals into actionable sub‑tasks, revises plans on‑the‑fly. | Prompt‑based planning, Tree‑of‑Thoughts, hierarchical task networks. | Robust error handling; plan‑repair loops. |\\n| **Short‑Term Memory (In‑Context)** | Holds recent dialogue, tool‑call context, and intermediate results. | Sliding‑window prompt, chunked prompts, prompt‑chunking. | Finite context window; formatting consistency. |\\n| **Long‑Term Memory (External Store)** | Persists knowledge across sessions, retrieves relevant facts. | Vector‑store (FAISS, Pinecone), knowledge graph, relational DB. | Retrieval quality vs. full‑attention trade‑off. |\\n| **Tool / API Interface** | Executes code, queries databases, calls external APIs. | Tool‑calling frameworks (LangChain, LlamaIndex, ReAct). | Reliability of natural‑language interface; parsing errors. |\\n| **Execution Engine** | Orchestrates tool calls, collects results, feeds back to the LLM. | Agent frameworks (AutoGPT, BabyAGI, GPT‑Engineer). | Asynchronous calls, error recovery. |\\n| **Reflection / Self‑Critique** | Evaluates past actions, learns from mistakes, refines future plans. | Self‑reflection prompts, separate evaluation LLM, RL‑HF loops. | Requires reliable feedback signals. |\\n| **Safety / Governance Layer** | Enforces constraints, prevents harmful behavior. | System messages, policy prompts, external guardrails. | Guard against “rebellious” outputs. |\\n| **Monitoring & Logging** | Tracks actions, performance, and logs for debugging and audit. | Structured logs, dashboards, audit trails. | Essential for debugging reliability issues. |\\n\\n### Why These Elements Matter\\n\\n| Element | Core Challenge it Addresses |\\n|--------|---------------------------|\\n| **LLM Controller** | Finite context limits; requires careful prompt‑engineering to keep coherence. |\\n| **Planning** | LLMs struggle to adjust plans when errors occur; dedicated planning mitigates brittleness. |\\n| **Reflection** | Enables autonomous improvement; needs reliable feedback. |\\n| **Memory** | Overcomes limited context window; vector stores provide larger knowledge pools but with weaker representation than full attention. |\\n| **Tool Interface** | Natural‑language interface is unreliable; parsing errors dominate current demos. |\\n| **Safety** | Prevents harmful or “rebellious” behavior. |\\n| **Monitoring** | Provides observability for debugging and audit. |\\n\\n### Design Flow\\n\\n1. **Goal Intake** – User or system supplies a high‑level goal.  \\n2. **Planning** – LLM decomposes the goal into sub‑tasks.  \\n3. **Execution Loop**  \\n   * LLM generates a tool‑call request.  \\n   * Execution Engine calls the tool, collects output.  \\n   * Output is fed back into the LLM (short‑term memory).  \\n4. **Reflection** – After each sub‑task, the agent self‑critiques and updates its plan.  \\n5. **Long‑Term Memory Update** – Persist new knowledge.  \\n6. **Safety Check** – Every output is filtered by the governance layer.  \\n7. **Monitoring** – All actions are logged for audit and debugging.\\n\\n### References\\n\\n1. Weng, L. (2023). *LLM‑powered Autonomous Agents*. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/  \\n2. Yao et\\u202fal. (2023). *Tree of Thoughts: Liberate Problem Solving with Large Language Models*. arXiv:2305.10601.  \\n3. Liu et\\u202fal. (2023). *Chain of Hindsight Aligns Language Models with Feedback*. arXiv:2302.02676.  \\n4. Liu et\\u202fal. (2023). *LLM+P: Empowering Large Language Models with Optimal Planning Proficiency*. arXiv:2304.11477.  \\n5. Shinn & Labash (2023). *Reflexion: an autonomous agent with dynamic memory and self‑reflection*. arXiv:2303.11366.  \\n\\nThese design elements collectively form the backbone of any robust, LLM‑driven autonomous agent system.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93081f3",
   "metadata": {},
   "source": [
    "### Answer individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb5e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6304\\2894720413.py:13: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
     ]
    }
   ],
   "source": [
    "# Answer each sub-question individually\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\":retrieved_docs, \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23584aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Main components of an LLM‑powered autonomous agent system**\\n\\n| # | Component | Core responsibilities |\\n|---|----------|----------------------|\\n| 1 | **Core LLM “brain”** | Generates plans, actions, and self‑reflection; serves as the agent’s decision‑making engine. |\\n| 2 | **Planning & reasoning** | Decomposes tasks into sub‑goals, refines plans (chain‑of‑thought, Tree‑of‑Thoughts, ReAct, etc.), and guides long‑term execution. |\\n| 3 | **Memory & retrieval** | Short‑term in‑context learning + long‑term external vector stores or knowledge bases that extend the LLM’s limited context window. |\\n| 4 | **Tool‑execution interface** | Parses the LLM’s natural‑language outputs into API calls or code execution, handling formatting and error mitigation. |\\n| 5 | **Self‑reflection & error handling** | Learns from past mistakes, adjusts plans, and incorporates safety or reward signals. |\\n\\nThese five pillars—brain, planning, memory, tool integration, and reflection—form the core architecture that enables an LLM‑powered autonomous agent to break down tasks, retain knowledge, interact with external systems, and improve over time.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is the set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (prompt | llm | StrOutputParser())\n",
    "final_rag_chain.invoke({\"context\":context, \"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
