{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c890cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, tiktoken, numpy as np, os\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "learn_api_key = os.environ['LANGCHAIN_API_KEY']\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "groq_api_key = os.environ['GROQ_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269aef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### INDEXING ###\n",
    "# loader = WebBaseLoader(\n",
    "#     web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# blog_docs = loader.load()\n",
    "\n",
    "# # Split\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# # Make Splits\n",
    "# splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# # Index\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     splits,\n",
    "#     HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "# )\n",
    "# retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846c213",
   "metadata": {},
   "source": [
    "### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba172c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given the user question choose which datasource would be the most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\", api_key=groq_api_key, temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a07b95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french)\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\":question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a39581c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here\n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here\n",
    "        return \"golang_docs\"\n",
    "    \n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213cb21",
   "metadata": {},
   "source": [
    "### Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690cf780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region in space where gravity is so strong that nothing, not even light, can escape. \n",
      "\n",
      "Imagine squeezing the entire mass of the sun into a space the size of a city. That's how incredibly dense a black hole is! This extreme density creates a gravitational pull so powerful that it warps the fabric of space and time around it. \n",
      "\n",
      "Anything that gets too close to a black hole gets pulled in and can never come out. We can't see black holes directly because they don't emit light, but we can detect them by observing their effects on nearby matter. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering question about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt\n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute Similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = ({\n",
    "    \"query\" : RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatGroq(model=\"gemma2-9b-it\", api_key=groq_api_key, temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
